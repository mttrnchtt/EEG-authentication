# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dQJFIVMLBTXDneVBIQm8fjkj-e-KmAzP
"""

!pip install -q gdown
!gdown 1NiJuC-WfmGOt7M0NxzqSU34s-2XZadVU
!gdown 1vSXbNpcDh6ODNvJn6SFUaN4JvsaAtkKr

!unzip /content/bbciRaw-20250419T162929Z-001.zip -d /content/unzipped_data/
!unzip /content/bbciRaw-20250419T162929Z-002.zip -d /content/unzipped_data/

# !unzip /content/drive/MyDrive/bbciRaw-20250419T162929Z-001.zip -d /content/unzipped_data/
# !unzip /content/drive/MyDrive/bbciRaw-20250419T162929Z-002.zip -d /content/unzipped_data/

!pip install mne

import os
import mne
import numpy as np
import shutil

def has_valid_eeg(vhdr_path):
    try:
        raw = mne.io.read_raw_brainvision(vhdr_path, preload=True, verbose=False)
        eeg_data = raw.get_data(picks='eeg')
        if eeg_data.size == 0 or np.all(eeg_data == 0):
            return False
        return True
    except Exception as e:
        print(f"âš ï¸ Error reading {vhdr_path}: {e}")
        return False

def scan_and_filter_eeg_dirs(base_dir):
    eeg_dirs = set()
    all_dirs_with_vhdr = set()
    total_vhdr = 0

    for root, _, files in os.walk(base_dir):
        for file in files:
            if file.endswith('.vhdr'):
                vhdr_path = os.path.join(root, file)
                total_vhdr += 1
                all_dirs_with_vhdr.add(root)
                if has_valid_eeg(vhdr_path):
                    print(f"âœ… EEG present: {vhdr_path}")
                    eeg_dirs.add(root)
                else:
                    print(f"âŒ No EEG data: {vhdr_path}")

    # Delete folders without valid EEG
    to_delete = all_dirs_with_vhdr - eeg_dirs
    for d in to_delete:
        try:
            print(f"ğŸ—‘ï¸ Deleting: {d}")
            shutil.rmtree(d)
        except Exception as e:
            print(f"âš ï¸ Could not delete {d}: {e}")

    print("\nğŸ“Š Summary:")
    print(f"Total .vhdr files scanned: {total_vhdr}")
    print(f"Total dirs with valid EEG: {len(eeg_dirs)}")
    print(f"Total dirs deleted: {len(to_delete)}")

# Replace with your actual folder path
scan_and_filter_eeg_dirs('/content/unzipped_data/bbciRaw')

import mne
import numpy as np
import pandas as pd
from glob import glob
import os

# â”€â”€ USER SETTINGS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
l_freq, h_freq       = 1, 40.0      # band-pass between 0.5â€“70 Hz
common_sfreq         = 250.0          # resample to 500 Hz
tmin, tmax           = -0.2, 1.0      # epoch window (seconds)
baseline             = (None, 0)      # baseline-correct from start to 0 s
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

vhdr_paths = glob('/content/unzipped_data/bbciRaw/*/*.vhdr')

for vhdr in vhdr_paths:
    folder = os.path.basename(os.path.dirname(vhdr))  # e.g., 'VPjak_15_07_20'
    user_id = folder.split('_')[0]                   # e.g., 'VPjak'
    session_date = '_'.join(folder.split('_')[1:])   # e.g., '15_07_20'

    print(f"ğŸ“¥ Loading {vhdr} (User: {user_id}, Date: {session_date})")

    # 1) Load raw EEG data
    raw = mne.io.read_raw_brainvision(vhdr, preload=True, verbose=False)

    # 2) Resample
    raw.resample(common_sfreq, npad='auto')

    # 3) Band-pass filter
    raw.filter(l_freq, h_freq, fir_design='firwin')

    # 4) Extract RSVP event markers
    events, event_id = mne.events_from_annotations(raw)

    # 5) Epoch the data
    epochs = mne.Epochs(
        raw,
        events,
        event_id=event_id,
        tmin=tmin,
        tmax=tmax,
        baseline=baseline,
        event_repeated='merge',
        preload=True,
        detrend=1
    )

    # 6) Attach user and session metadata
    epochs.metadata = pd.DataFrame({
        'user': [user_id] * len(epochs),
        'session': [session_date] * len(epochs)
    })

    # 7) Summary
    n_epochs, n_channels, n_times = epochs.get_data().shape
    print(f"âœ… {n_epochs} epochs Ã— {n_channels} ch Ã— {n_times} samples")

    # 8) Save to file
    out_fname = vhdr.replace(
        '.vhdr',
        f'_epo_{int((tmax-tmin)*1000)}ms_{int(common_sfreq)}Hz.fif'
    )
    epochs.save(out_fname, overwrite=True)
    print(f"ğŸ’¾ Saved to {out_fname}\n")

print("ğŸ‰ All subjects processed.")

import mne
import numpy as np
import pandas as pd
from glob import glob

# â”€â”€ CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
data_glob    = '/content/unzipped_data/bbciRaw/**/*.fif'
desired_nchan = 16
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Find and process only the 16â€‘channel files
epoch_files = glob(data_glob, recursive=True)
data_list, meta_list = [], []
file_count = 0

for f in epoch_files:
    try:
        epochs = mne.read_epochs(f, preload=True, verbose=False)
        # skip any file that isn't exactly 16 channels
        if epochs.info['nchan'] != desired_nchan:
            epochs.close()
            continue

        # at this point, epochs.ch_names is your full 16â€‘channel set
        data_list.append(epochs.get_data())   # shape: (n_epochs, 16, n_times)
        meta_list.append(epochs.metadata)     # DataFrame with 'user' & 'session'

        file_count += 1
        epochs.close()

    except Exception as e:
        print(f"Skipping {f}: {e}")

if file_count == 0:
    raise RuntimeError(f"No {desired_nchan}-channel files found!")

# Concatenate across files
X = np.concatenate(data_list, axis=0)      # â†’ (total_epochs, 16, n_times)
metadata = pd.concat(meta_list, ignore_index=True)

# Labels & groups for Siamese/Triplet
y      = metadata['user'].to_numpy()       # user strings (or map to ints later)
groups = metadata['session'].to_numpy()    # session IDs if you need grouping

print(f"Processed {file_count} files â†’ X shape = {X.shape}, epochs = {len(metadata)}")

# from sklearn.model_selection import train_test_split
# from sklearn.preprocessing import LabelEncoder
# import numpy as np

# le = LabelEncoder()
# y_int = le.fit_transform(metadata['user'].to_numpy())
# metadata['user_idx'] = y_int  # Optional: adds int labels to metadata for inspection

# # 2) Split users into train vs. holdout (e.g., 80/20 split)
# all_users = np.unique(y_int)
# train_users, holdout_users = train_test_split(all_users, test_size=0.2, random_state=42)

# # 3) Create boolean masks
# train_mask = np.isin(y_int, train_users)
# hold_mask  = np.isin(y_int, holdout_users)

# # 4) Apply masks to features and labels
# X_train, y_train = X[train_mask], y_int[train_mask]
# X_hold,  y_hold  = X[hold_mask],  y_int[hold_mask]

import numpy as np
from sklearn.model_selection import GroupShuffleSplit
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score, roc_auc_score
from collections import Counter

# Assuming you have already extracted:
# X (n_samples Ã— n_features)
# y (user labels)
# groups (session identifiers)

# Step 1: Convert to binary classification problem
unique_users = np.unique(y)
if len(unique_users) < 2:
    raise ValueError("Need at least two users for authentication")

# Select target user (most frequent for demonstration)
user_counts = Counter(y)
target_user = user_counts.most_common(1)[0][0]
print(f"Target user: {target_user} with {user_counts[target_user]} samples")

# Create binary labels
y_binary = (y == target_user).astype(int)

# Step 2: Memory-efficient train-test split
splitter = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
train_idx, test_idx = next(splitter.split(X, y_binary, groups))

# Create arrays without copying data
X_reshaped = X.reshape(X.shape[0], -1)
X_train = X_reshaped[train_idx]
X_test = X_reshaped[test_idx]
y_train = y_binary[train_idx]
y_test = y_binary[test_idx]

# Step 3: Handle class imbalance
print("Class distribution:")
print("Train:", Counter(y_train))
print("Test:", Counter(y_test))

# # Step 4: Train authentication model with memory optimization
# clf = SVC(
#     class_weight='balanced',
#     probability=True,
#     cache_size=500  # Adjust based on your system's memory (MB)
# )

# # Use incremental scaling to avoid memory spikes
# scaler = StandardScaler()
# X_train_scaled = scaler.fit_transform(X_train.astype(np.float32))
# clf.fit(X_train_scaled, y_train)

# # Evaluate
# X_test_scaled = scaler.transform(X_test.astype(np.float32))
# y_pred = clf.predict(X_test_scaled)

# print("\nAuthentication Results:")
# print(classification_report(y_test, y_pred, target_names=['Imposter', 'Genuine']))

# # Memory cleanup
# del X_train, X_test, X_train_scaled, X_test_scaled

import numpy as np
from scipy.signal            import welch
from statsmodels.tsa.ar_model import AutoReg
from scipy.stats             import skew, kurtosis
from sklearn.pipeline        import make_pipeline, FeatureUnion, FunctionTransformer
from sklearn.preprocessing   import StandardScaler
from sklearn.svm             import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics         import (
    classification_report,
    accuracy_score,
    roc_auc_score
)
from collections import Counter

# ĞŸÑ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ĞµĞ¼, Ñ‡Ñ‚Ğ¾ X, y, groups Ğ¸ common_sfreq ÑƒĞ¶Ğµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ñ‹ Ñ€Ğ°Ğ½ĞµĞµ
sfreq = common_sfreq   # Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ° Ğ´Ğ»Ñ featureâ€‘Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹

# â”€â”€â”€ featureâ€‘extraction + pipeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def bandpower(epoch, sf=sfreq, bands=[(1,4),(4,8),(8,13),(13,30),(30,45)]):
    f, Pxx = welch(epoch, fs=sf, nperseg=sf)
    return np.hstack([Pxx[:, (f>=l)&(f<h)].mean(axis=1) for l,h in bands])

def ar_feats(epoch, p=6):
    coeffs = []
    for ch in epoch:
        m = AutoReg(ch, lags=p, old_names=False).fit()
        coeffs.append(m.params[1:])
    return np.hstack(coeffs)

def hjorth(epoch):
    feats = []
    for ch in epoch:
        d1, d2 = np.diff(ch), np.diff(np.diff(ch))
        v0, v1, v2 = ch.var(), d1.var(), d2.var()
        mobility = np.sqrt(v1/v0)
        complexity = np.sqrt(v2/v1)/mobility
        feats.append([v0, mobility, complexity])
    return np.hstack(feats)

def stats_feats(epoch):
    feats = []
    for ch in epoch:
        feats.append([ch.mean(), ch.std(), skew(ch), kurtosis(ch)])
    return np.hstack(feats)

pipe = make_pipeline(
    FeatureUnion([
      ('bp', FunctionTransformer(lambda X: np.vstack([bandpower(e)   for e in X]))),
      ('ar', FunctionTransformer(lambda X: np.vstack([ar_feats(e)    for e in X]))),
      ('hj', FunctionTransformer(lambda X: np.vstack([hjorth(e)      for e in X]))),
      ('st', FunctionTransformer(lambda X: np.vstack([stats_feats(e)  for e in X]))),
    ]),
    StandardScaler(),
    SVC(kernel='rbf', class_weight='balanced', probability=True)
)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# â”€â”€â”€ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ y â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
target_user = Counter(y).most_common(1)[0][0]
y_binary   = (y == target_user).astype(int)
print("Overall distribution:", Counter(y_binary))

# â”€â”€â”€ stratified train/test split â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
X_train, X_test, y_train, y_test = train_test_split(
    X, y_binary,
    test_size=0.2,
    random_state=42,
    stratify=y_binary
)
print("Train distribution:", Counter(y_train))
print("Test  distribution:", Counter(y_test))

# â”€â”€â”€ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
pipe.fit(X_train, y_train)
y_pred = pipe.predict(X_test)
y_proba = pipe.predict_proba(X_test)[:,1]

print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=['Imposter','Genuine']))
print(f"Accuracy : {accuracy_score(y_test, y_pred):.2f}")
print(f"AUC-ROC  : {roc_auc_score(y_test, y_proba):.2f}")


# Step 1: Convert to binary classification
#unique_users = np.unique(y)
#if len(unique_users) < 2:
    #raise ValueError("Need at least two users for authentication")

#target_user = Counter(y).most_common(1)[0][0]
#y_binary = (y == target_user).astype(int)

# Stratified group splitting function
#def stratified_group_split(y, groups, test_size=0.2, random_state=42):
    #splitter = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)

    # Split positive and negative classes separately
    #splits = []
    #for class_label in [0, 1]:
        #class_idx = np.where(y == class_label)[0]
        #class_groups = groups[class_idx]

        #try:
            # Attempt split for this class
            #train, test = next(splitter.split(class_idx, groups=class_groups))
            #splits.append((class_idx[train], class_idx[test]))
        #except ValueError:
            # Handle cases with insufficient groups
            #splits.append((class_idx, np.array([], dtype=int)))

    # Combine splits from both classes
    #train_idx = np.concatenate([splits[0][0], splits[1][0]])
    #test_idx = np.concatenate([splits[0][1], splits[1][1]])

    #return np.random.permutation(train_idx), np.random.permutation(test_idx)

# Perform stratified group split
#train_idx, test_idx = stratified_group_split(y_binary, groups)

# Prepare datasets
#X_train, X_test = X[train_idx], X[test_idx]
#y_train, y_test = y_binary[train_idx], y_binary[test_idx]

# Verify class distribution
#print("Train distribution:", Counter(y_train))
#print("Test distribution:", Counter(y_test))

# # Train model with class balancing
# clf = make_pipeline(
#     StandardScaler(),
#     SVC(class_weight='balanced', probability=True)
# )

# clf.fit(X_train, y_train)
# y_pred = clf.predict(X_test)

# # Show results
# print("\nAuthentication Report:")
# print(classification_report(y_test, y_pred, target_names=['Imposter', 'Genuine']))

import numpy as np

values, counts = np.unique(y_test, return_counts=True)
for v, c in zip(values, counts):
    print(f"{v}: {c}")

!pip install lightgbm

import numpy as np
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.pipeline import make_pipeline
from sklearn.decomposition import TruncatedSVD
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_classification

# Example data (replace with your data)
# X, y = make_classification(n_samples=10000, n_features=8000, random_state=42)

# Optimal logistic regression pipeline
pipeline = make_pipeline(
    StandardScaler(),  # Critical for convergence speed
    TruncatedSVD(n_components=300, algorithm='randomized'),  # Reduce to 300 features
    LogisticRegression(
        penalty='l1',
        solver='saga',     # Fastest for large datasets
        C=1,             # Regularization strength
        max_iter=5000,
        n_jobs=-1,         # Parallelize coordinate descent
        verbose=0
    )
)
X_train = X_train.reshape(X_train.shape[0], -1)
X_test = X_test.reshape(X_test.shape[0], -1)

# Fit and evaluate
pipeline.fit(X_train, y_train)
print(f"Accuracy: {pipeline.score(X_test, y_test):.2f}")

y_pred = pipeline.predict(X_test)
y_proba = pipeline.predict_proba(X_test)[:, 1]

# Generate report
print("Classification Report:")
print(classification_report(y_test, y_pred, target_names=['Imposter', 'Genuine']))

# Additional metrics
print(f"\nAccuracy: {accuracy_score(y_test, y_pred):.2f}")
print(f"AUC-ROC: {roc_auc_score(y_test, y_proba):.2f}")

# Use dimensionality reduction
from sklearn.decomposition import PCA

pca = PCA(n_components=0.95, whiten=True)
X_train_pca = pca.fit_transform(X_train)
X_test_pca = pca.transform(X_test)

clf = SVC(
    class_weight='balanced',
    probability=True,
    # cache_size=500  # Adjust based on system's memory (MB)
)

# Use incremental scaling to avoid memory spikes
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train.astype(np.float32))
clf.fit(X_train_scaled, y_train)

# Evaluate
X_test_scaled = scaler.transform(X_test.astype(np.float32))
y_pred = clf.predict(X_test_scaled)

print("\nAuthentication Results:")
print(classification_report(y_test, y_pred, target_names=['Imposter', 'Genuine']))

# import mne
# import numpy as np
# from glob import glob

# # 1) Find all epoch files
# files = glob('/content/unzipped_data/bbciRaw/**/*.fif', recursive=True)

# fixed_epochs = []
# for f in files:
#     try:
#         # Read epochs with preload
#         ep = mne.read_epochs(f, preload=True, verbose=False)
#         n_events = len(ep.events)
#         n_meta = len(ep.metadata) if ep.metadata is not None else 0

#         # If metadata length exceeds number of events, truncate it
#         if ep.metadata is not None and n_meta > n_events:
#             ep.metadata = ep.metadata.iloc[:n_events].reset_index(drop=True)

#         fixed_epochs.append(ep)
#         print(f"[OK] {f} â€” epochs: {n_events}, metadata: {n_meta}")
#     except Exception as e:
#         print(f"[SKIP] {f} â€” error: {e}")

# # 2) Concatenate all fixed epochs
# combined_epochs = mne.concatenate_epochs(fixed_epochs)

# # 3) Extract data and metadata
# X = combined_epochs.get_data()                                # shape: (n_epochs, n_channels, n_times)
# y = combined_epochs.metadata['user'].to_numpy()               # user labels
# groups = combined_epochs.metadata['session'].to_numpy()       # session labels

# # 4) Optionally flatten for typical sklearn models
# X_flat = X.reshape(X.shape[0], -1)                            # shape: (n_epochs, n_channels * n_times)

# print(f"Total epochs: {X.shape[0]}, channels: {X.shape[1]}, time points: {X.shape[2]}")
# print("X_flat shape:", X_flat.shape)

# import mne
# import numpy as np
# from glob import glob
# from joblib import Parallel, delayed
# from tqdm.notebook import tqdm
# import pandas as pd
# import gc

# def process_single_file(f):
#     """Process a single epoch file"""
#     try:
#         # Read epochs with preload
#         ep = mne.read_epochs(f, preload=True, verbose=False)
#         n_events = len(ep.events)
#         n_meta = len(ep.metadata) if ep.metadata is not None else 0
#         n_channels = len(ep.ch_names)

#         # If metadata length exceeds number of events, truncate it
#         if ep.metadata is not None and n_meta > n_events:
#             ep.metadata = ep.metadata.iloc[:n_events].reset_index(drop=True)

#         # Extract necessary data and clear memory
#         data = ep.get_data()
#         metadata = ep.metadata.copy() if ep.metadata is not None else None
#         del ep
#         gc.collect()

#         return {
#             'data': data,
#             'metadata': metadata,
#             'status': 'OK',
#             'file': f,
#             'n_events': n_events,
#             'n_meta': n_meta,
#             'n_channels': n_channels
#         }

#     except Exception as e:
#         return {
#             'status': 'ERROR',
#             'file': f,
#             'error': str(e)
#         }

# def load_and_combine_epochs(data_path, n_jobs=-1):
#     """Load and combine epochs in parallel"""

#     # 1) Find all epoch files
#     files = glob(data_path, recursive=True)
#     print(f"Found {len(files)} files")

#     # 2) Process files in parallel with progress bar
#     results = Parallel(n_jobs=n_jobs)(
#         delayed(process_single_file)(f) for f in tqdm(files, desc="Processing files")
#     )

#     # 3) Separate successful and failed processes
#     successful = [r for r in results if r['status'] == 'OK']
#     failed = [r for r in results if r['status'] == 'ERROR']

#     # Print statistics
#     print(f"\nSuccessfully processed: {len(successful)} files")
#     print(f"Failed: {len(failed)} files")

#     if failed:
#         print("\nFailed files:")
#         for f in failed:
#             print(f"[SKIP] {f['file']} â€” error: {f['error']}")

#     # 4) Group by channel count
#     channel_groups = {}
#     for result in successful:
#         n_channels = result['data'].shape[1]
#         if n_channels not in channel_groups:
#             channel_groups[n_channels] = []
#         channel_groups[n_channels].append(result)

#     # Print channel count statistics
#     print("\nChannel count distribution:")
#     for n_channels, group in channel_groups.items():
#         print(f"{n_channels} channels: {len(group)} files")

#     # 5) Process the largest group
#     max_channel_count = max(channel_groups.keys(), key=lambda k: len(channel_groups[k]))
#     print(f"\nProcessing files with {max_channel_count} channels (largest group)")

#     # 6) Combine data and metadata for the selected channel group
#     selected_results = channel_groups[max_channel_count]
#     all_data = []
#     all_metadata = []

#     for result in selected_results:
#         all_data.append(result['data'])
#         if result['metadata'] is not None:
#             all_metadata.append(result['metadata'])

#     # 7) Concatenate data and metadata
#     X = np.concatenate(all_data, axis=0)

#     if all_metadata:
#         metadata = pd.concat(all_metadata, ignore_index=True)
#         y = metadata['user'].to_numpy()
#         groups = metadata['session'].to_numpy()
#     else:
#         metadata = None
#         y = None
#         groups = None

#     # 8) Clear memory
#     del all_data, all_metadata
#     gc.collect()

#     return X, y, groups, metadata

# # Usage
# X, y, groups, metadata = load_and_combine_epochs(
#     data_path='/content/unzipped_data/bbciRaw/**/*.fif',
#     n_jobs=-1  # Use all available cores
# )

# # Create flattened version if needed
# X_flat = X.reshape(X.shape[0], -1)

# print(f"\nFinal dataset:")
# print(f"Total epochs: {X.shape[0]}, channels: {X.shape[1]}, time points: {X.shape[2]}")
# print("X_flat shape:", X_flat.shape)

import numpy as np
from scipy.signal      import welch
from statsmodels.tsa.ar_model import AutoReg
from scipy.stats      import skew, kurtosis
from sklearn.pipeline  import make_pipeline, FeatureUnion, FunctionTransformer
from sklearn.preprocessing import StandardScaler
from sklearn.svm       import SVC

sfreq = 250

def bandpower(epoch, sf=sfreq, bands=[(1,4),(4,8),(8,13),(13,30),(30,45)]):
    f, Pxx = welch(epoch, fs=sfreq, nperseg=sfreq)
    return np.hstack([Pxx[:, (f>=l)&(f<h)].mean(axis=1) for l,h in bands])

def ar_feats(epoch, p=6):
    coeffs = []
    for ch in epoch:
        model = AutoReg(ch, lags=p, old_names=False).fit()
        coeffs.append(model.params[1:])
    return np.hstack(coeffs)

def hjorth(epoch):
    feats = []
    for ch in epoch:
        d1 = np.diff(ch); d2 = np.diff(d1)
        v0, v1, v2 = ch.var(), d1.var(), d2.var()
        m = np.sqrt(v1/v0); c = np.sqrt(v2/v1)/m
        feats.append([v0,m,c])
    return np.hstack(feats)

def stats_feats(epoch):
    feats = []
    for ch in epoch:
        feats.append([ch.mean(), ch.std(), skew(ch), kurtosis(ch)])
    return np.hstack(feats)

fe_union = FeatureUnion([
    ('bp', FunctionTransformer(lambda X: np.vstack([bandpower(e)   for e in X]))),
    ('ar', FunctionTransformer(lambda X: np.vstack([ar_feats(e)    for e in X]))),
    ('hj', FunctionTransformer(lambda X: np.vstack([hjorth(e)      for e in X]))),
    ('st', FunctionTransformer(lambda X: np.vstack([stats_feats(e)  for e in X])))
])
pipe = make_pipeline(
    fe_union,
    StandardScaler(),
    SVC(kernel='rbf', class_weight='balanced')
)

# import os
# import mne
# import numpy as np
# from tqdm import tqdm

# def get_label_from_path(path, base_dir):
#     """Extract label as the name of the direct subdirectory under base_dir."""
#     rel_path = os.path.relpath(path, base_dir)
#     label = rel_path.split(os.sep)[0]  # take the immediate folder name
#     return label

# def find_vhdr_files(base_dir):
#     vhdr_paths = []
#     for root, _, files in os.walk(base_dir):
#         for file in files:
#             if file.endswith('.vhdr'):
#                 vhdr_paths.append(os.path.join(root, file))
#     return vhdr_paths

# def extract_epochs(vhdr_path, tmin=-0.2, tmax=0.8):
#     raw = mne.io.read_raw_brainvision(vhdr_path, preload=True, verbose=False)
#     events, event_id = mne.events_from_annotations(raw, verbose=False)

#     if len(events) == 0:
#         return None

#     epochs = mne.Epochs(raw, events, event_id=event_id, tmin=tmin, tmax=tmax,
#                         baseline=(None, 0), picks='eeg', preload=True, verbose=False)
#     return epochs.get_data()  # shape: (n_epochs, n_channels, n_times)

# def build_dataset(base_dir):
#     vhdr_files = find_vhdr_files(base_dir)

#     X = []  # data
#     y = []  # labels
#     label_set = set()

#     for vhdr_path in tqdm(vhdr_files, desc="Building dataset"):
#         label = get_label_from_path(vhdr_path, base_dir)
#         label_set.add(label)

#         try:
#             epoch_data = extract_epochs(vhdr_path)
#             if epoch_data is not None:
#                 X.append(epoch_data)
#                 y.extend([label] * epoch_data.shape[0])
#         except Exception as e:
#             print(f"Error processing {vhdr_path}: {e}")

#     # Convert to numpy arrays
#     X = np.concatenate(X, axis=0)  # (total_epochs, n_channels, n_times)
#     y = np.array(y)

#     print(f"\nâœ… Final dataset shape: {X.shape}, labels: {len(np.unique(y))} classes: {np.unique(y)}")
#     return X, y

# # Example usage
# base_dir = '/content/unzipped_data/bbciRaw'
# X, y = build_dataset(base_dir)

from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.mixture import GaussianMixture
from sklearn.metrics import f1_score, classification_report
import numpy as np
from scipy.stats import skew, kurtosis
from scipy.signal import welch

def extract_features(X):
    """
    Extract multiple features from the EEG signals
    X shape: (n_epochs, n_channels, n_times)
    """
    n_epochs, n_channels, n_times = X.shape
    features = []

    for epoch in range(n_epochs):
        epoch_features = []

        for channel in range(n_channels):
            signal = X[epoch, channel]

            # Time domain features
            # 1. Mean
            mean = np.mean(signal)

            # 2. Standard deviation
            std = np.std(signal)

            # 3. Average Amplitude Change (AAC)
            aac = np.mean(np.abs(np.diff(signal)))

            # 4. Zero Crossing Rate (ZCR)
            zcr = np.sum(np.diff(np.signbit(signal).astype(int)) != 0) / (len(signal) - 1)

            # 5. Root Mean Square (RMS)
            rms = np.sqrt(np.mean(signal**2))

            # 6. Mobility (Hjorth parameter)
            diff1 = np.diff(signal)
            mobility = np.std(diff1) / np.std(signal)

            # 7. Complexity (Hjorth parameter)
            diff2 = np.diff(diff1)
            complexity = (np.std(diff2) * np.std(signal)) / (np.std(diff1) ** 2)

            # 8. Skewness
            sk = skew(signal)

            # 9. Kurtosis
            kurt = kurtosis(signal)

            # Frequency domain features
            # 10. Power Spectral Density (PSD) bands
            freqs, psd = welch(signal, fs=250, nperseg=min(256, len(signal)))

            # Delta (0.5-4 Hz)
            delta_idx = np.logical_and(freqs >= 0.5, freqs <= 4)
            delta_power = np.mean(psd[delta_idx])

            # Theta (4-8 Hz)
            theta_idx = np.logical_and(freqs >= 4, freqs <= 8)
            theta_power = np.mean(psd[theta_idx])

            # Alpha (8-13 Hz)
            alpha_idx = np.logical_and(freqs >= 8, freqs <= 13)
            alpha_power = np.mean(psd[alpha_idx])

            # Beta (13-30 Hz)
            beta_idx = np.logical_and(freqs >= 13, freqs <= 30)
            beta_power = np.mean(psd[beta_idx])

            # Gamma (30-100 Hz)
            gamma_idx = np.logical_and(freqs >= 30, freqs <= 100)
            gamma_power = np.mean(psd[gamma_idx])

            # Combine all features
            channel_features = [
                mean, std, aac, zcr, rms, mobility, complexity, sk, kurt,
                delta_power, theta_power, alpha_power, beta_power, gamma_power
            ]

            epoch_features.extend(channel_features)

        features.append(epoch_features)

    return np.array(features)

# Extract features from the raw EEG data
X_features = extract_features(X)

# Initialize results dictionary to store scores for each user
user_scores = {}

# For each unique user
for target_user in np.unique(y):
    print(f"\n=== Testing for user: {target_user} ===")

    # Create binary labels (1 for target user, 0 for others)
    y_binary = (y == target_user).astype(int)

    # Print class distribution
    print("Class distribution:")
    unique, counts = np.unique(y_binary, return_counts=True)
    for cls, count in zip(unique, counts):
        print(f"Class {cls}: {count} samples ({count/len(y_binary)*100:.1f}%)")

    # Initialize models
    scaler = StandardScaler()

    # Perform cross-validation
    n_splits = 5
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
    accuracy_scores = []
    f1_macro_scores = []

    for fold_idx, (train_idx, test_idx) in enumerate(skf.split(X_features, y_binary)):
        X_train = X_features[train_idx]
        X_test = X_features[test_idx]
        y_train = y_binary[train_idx]
        y_test = y_binary[test_idx]

        # Scale the features
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        # Train GMM on genuine user data only
        genuine_data = X_train_scaled[y_train == 1]
        gmm = GaussianMixture(n_components=2, random_state=42)
        gmm.fit(genuine_data)

        # Predict using likelihood threshold
        log_probs = gmm.score_samples(X_test_scaled)
        threshold = np.percentile(gmm.score_samples(genuine_data), 5)  # 5th percentile threshold
        y_pred = (log_probs >= threshold).astype(int)

        # Calculate metrics
        accuracy = np.mean(y_pred == y_test)
        f1_macro = f1_score(y_test, y_pred, average='macro')

        accuracy_scores.append(accuracy)
        f1_macro_scores.append(f1_macro)

        print(f"\nFold {fold_idx + 1}:")
        print(f"Accuracy: {accuracy:.3f}")
        print(f"F1-macro: {f1_macro:.3f}")
        print("\nDetailed metrics:")
        print(classification_report(y_test, y_pred))

        del X_train, X_test, X_train_scaled, X_test_scaled

    accuracy_scores = np.array(accuracy_scores)
    f1_macro_scores = np.array(f1_macro_scores)

    user_scores[target_user] = {
        'accuracy': accuracy_scores,
        'f1_macro': f1_macro_scores
    }

    print(f"\nUser {target_user} results:")
    print(f"Mean accuracy: {accuracy_scores.mean():.3f} (+/- {accuracy_scores.std() * 2:.3f})")
    print(f"Mean F1-macro: {f1_macro_scores.mean():.3f} (+/- {f1_macro_scores.std() * 2:.3f})")

# Print overall results
print("\n=== Overall Results ===")
all_accuracies = np.concatenate([scores['accuracy'] for scores in user_scores.values()])
all_f1_macros = np.concatenate([scores['f1_macro'] for scores in user_scores.values()])

print(f"Mean accuracy across all users: {all_accuracies.mean():.3f} (+/- {all_accuracies.std() * 2:.3f})")
print(f"Mean F1-macro across all users: {all_f1_macros.mean():.3f} (+/- {all_f1_macros.std() * 2:.3f})")

# Print individual user mean scores
print("\nMean scores for each user:")
for user, scores in user_scores.items():
    print(f"\nUser {user}:")
    print(f"Accuracy: {scores['accuracy'].mean():.3f}")
    print(f"F1-macro: {scores['f1_macro'].mean():.3f}")